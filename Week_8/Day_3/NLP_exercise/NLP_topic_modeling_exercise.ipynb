{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "NLP_topic_modeling_exercise.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-29T12:18:12.932082Z",
          "start_time": "2020-04-29T12:18:12.200358Z"
        },
        "id": "CFU0J4b6KJIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import TfidfVectorizer and CountVectorizer from sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "# import fetch_20newsgroups from sklearn.datasets\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# import NMF and LatentDirichletAllocation from sklearn\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-29T12:18:14.463840Z",
          "start_time": "2020-04-29T12:18:13.020189Z"
        },
        "id": "IXgPws3rKJIr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "225ca651-0355-485a-dcb8-bb07273ebfda"
      },
      "source": [
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
        "documents = dataset.data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q3u9AarKJI9",
        "colab_type": "text"
      },
      "source": [
        "* create variable 'no_features' and set its value to 100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-29T12:18:15.590700Z",
          "start_time": "2020-04-29T12:18:15.585820Z"
        },
        "id": "3mrBGLtMKJI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "no_features = 100"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpxlryrJKJJO",
        "colab_type": "text"
      },
      "source": [
        "* create variable 'no_topics' and set its value to 100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-29T12:18:16.743304Z",
          "start_time": "2020-04-29T12:18:16.737763Z"
        },
        "id": "pPdWOF4jKJJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "no_topics = 100"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrVtjfLLKJJd",
        "colab_type": "text"
      },
      "source": [
        "# NMF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csxNE1cvKJJg",
        "colab_type": "text"
      },
      "source": [
        "* instantiate TfidfVectorizer with following params:\n",
        "* max_df=0.95, min_df=2, max_features=no_features, stop_words='english'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-29T12:18:17.892838Z",
          "start_time": "2020-04-29T12:18:17.888668Z"
        },
        "id": "oRRVt6KgKJJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = TfidfVectorizer(max_df = 0.95, min_df=2, max_features=no_features,stop_words='english')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeyLGsRTKJJy",
        "colab_type": "text"
      },
      "source": [
        "* use fit_transform method of TfidfVectorizer to transform documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-29T12:18:21.486038Z",
          "start_time": "2020-04-29T12:18:19.135830Z"
        },
        "id": "0qrmISCHKJJ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "86a9a45a-30c8-4ee4-8694-bf2b187a6ac7"
      },
      "source": [
        "docs_vectorized = vectorizer.fit_transform(documents)\n",
        "docs_vectorized"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<11314x100 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 83200 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrVuYvU2KJJ7",
        "colab_type": "text"
      },
      "source": [
        "* get features names from TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-29T12:18:22.661253Z",
          "start_time": "2020-04-29T12:18:22.656169Z"
        },
        "id": "h-qv74TBKJJ8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "9b72db2e-fc95-4208-8bd6-8b7f454ce6b0"
      },
      "source": [
        "feat_names = vectorizer.get_feature_names()\n",
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['00', '10', '12', '14', '15', '16', '20', '25', 'a86', 'available', 'ax', 'b8f', 'believe', 'best', 'better', 'bit', 'case', 'com', 'come', 'course', 'data', 'day', 'did', 'didn', 'different', 'does', 'doesn', 'don', 'drive', 'edu', 'fact', 'far', 'file', 'g9v', 'god', 'going', 'good', 'got', 'government', 'help', 'information', 'jesus', 'just', 'key', 'know', 'law', 'let', 'like', 'line', 'list', 'little', 'll', 'long', 'look', 'lot', 'mail', 'make', 'max', 'mr', 'need', 'new', 'number', 'people', 'point', 'power', 'probably', 'problem', 'program', 'question', 'read', 'really', 'right', 'run', 'said', 'say', 'second', 'set', 'software', 'space', 'state', 'sure', 'tell', 'thanks', 'thing', 'things', 'think', 'time', 'true', 'try', 'use', 'used', 'using', 've', 'want', 'way', 'windows', 'work', 'world', 'year', 'years']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6u8pNpfKJKC",
        "colab_type": "text"
      },
      "source": [
        "* instantiate NMF and fit transformed data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-29T12:18:24.532755Z",
          "start_time": "2020-04-29T12:18:23.661009Z"
        },
        "id": "P1j55thOKJKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nmf = NMF(n_components=no_topics)\n",
        "nmf_tf = nmf.fit_transform(docs_vectorized)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fCMRZQoKJKN",
        "colab_type": "text"
      },
      "source": [
        "# LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNVDkHR1KJKP",
        "colab_type": "text"
      },
      "source": [
        "* instantiate CountVectorizer with following params:\n",
        "* max_df=0.95, min_df=2, max_features=no_features, stop_words='english'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-29T12:18:25.547906Z",
          "start_time": "2020-04-29T12:18:25.540452Z"
        },
        "id": "JBlZ4sY3KJKR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cvec = CountVectorizer(max_df=0.95,min_df=2,max_features=no_features,stop_words='english')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOAzdeM7KJKZ",
        "colab_type": "text"
      },
      "source": [
        "* use fit_transform method of CountVectorizer to transform documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-29T12:18:29.307223Z",
          "start_time": "2020-04-29T12:18:26.637153Z"
        },
        "id": "8nYb2_lCKJKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs_vectorized2 = cvec.fit_transform(documents)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9pVsJqeKJKj",
        "colab_type": "text"
      },
      "source": [
        "* get features names from TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-29T12:18:31.516381Z",
          "start_time": "2020-04-29T12:18:31.498740Z"
        },
        "id": "UXCno7YSKJKm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "dc8b7e70-5c75-48d3-ec22-e7ebea54e8fb"
      },
      "source": [
        "feat_names2 = cvec.get_feature_names()\n",
        "print(feat_names2)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['00', '10', '12', '14', '15', '16', '20', '25', 'a86', 'available', 'ax', 'b8f', 'believe', 'best', 'better', 'bit', 'case', 'com', 'come', 'course', 'data', 'day', 'did', 'didn', 'different', 'does', 'doesn', 'don', 'drive', 'edu', 'fact', 'far', 'file', 'g9v', 'god', 'going', 'good', 'got', 'government', 'help', 'information', 'jesus', 'just', 'key', 'know', 'law', 'let', 'like', 'line', 'list', 'little', 'll', 'long', 'look', 'lot', 'mail', 'make', 'max', 'mr', 'need', 'new', 'number', 'people', 'point', 'power', 'probably', 'problem', 'program', 'question', 'read', 'really', 'right', 'run', 'said', 'say', 'second', 'set', 'software', 'space', 'state', 'sure', 'tell', 'thanks', 'thing', 'things', 'think', 'time', 'true', 'try', 'use', 'used', 'using', 've', 'want', 'way', 'windows', 'work', 'world', 'year', 'years']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4bEBt3dKJK6",
        "colab_type": "text"
      },
      "source": [
        "* instantiate LatentDirichletAllocation and fit transformed data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-29T12:19:03.315322Z",
          "start_time": "2020-04-29T12:18:32.768365Z"
        },
        "id": "D769Szt6KJK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda_model = LatentDirichletAllocation(n_components=no_topics,max_iter=10,learning_method='online')\n",
        "lda_nmz = lda_model.fit_transform(docs_vectorized2)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJr5W0lDKJLC",
        "colab_type": "text"
      },
      "source": [
        "* create a function display_topics that is able to display the top words in a topic for different models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-29T12:19:04.476192Z",
          "start_time": "2020-04-29T12:19:04.469045Z"
        },
        "id": "YRc4hZNNKJLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_topics(model, vectorizer, top_n=10):\n",
        "    for idx, topic in enumerate(model.components_):\n",
        "        print(\"Topic %d:\" % (idx))\n",
        "        #print([(vectorizer.get_feature_names()[i], topic[i]) for i in topic.argsort()[:-top_n - 1:-1]])\n",
        "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
        "                        for i in topic.argsort()[0:top_n]])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wjmCZvGKJLN",
        "colab_type": "text"
      },
      "source": [
        "* display top 1o words from each topic from NMF model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-29T12:19:05.672461Z",
          "start_time": "2020-04-29T12:19:05.656545Z"
        },
        "id": "Ix9VwcstKJLO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "333c0216-0dbc-4dfb-929c-4e767f9cad67"
      },
      "source": [
        "print(\"NMF Model:\")\n",
        "print_topics(nmf, vectorizer, top_n=1)\n",
        "print(\"=\" * 20)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NMF Model:\n",
            "Topic 0:\n",
            "[('00', 0.0)]\n",
            "Topic 1:\n",
            "[('00', 0.0)]\n",
            "Topic 2:\n",
            "[('00', 0.0)]\n",
            "Topic 3:\n",
            "[('00', 0.0)]\n",
            "Topic 4:\n",
            "[('00', 0.0)]\n",
            "Topic 5:\n",
            "[('00', 0.0)]\n",
            "Topic 6:\n",
            "[('00', 0.0)]\n",
            "Topic 7:\n",
            "[('00', 0.0)]\n",
            "Topic 8:\n",
            "[('00', 0.0)]\n",
            "Topic 9:\n",
            "[('00', 0.0)]\n",
            "Topic 10:\n",
            "[('00', 0.0)]\n",
            "Topic 11:\n",
            "[('00', 0.0)]\n",
            "Topic 12:\n",
            "[('00', 0.0)]\n",
            "Topic 13:\n",
            "[('00', 0.0)]\n",
            "Topic 14:\n",
            "[('00', 0.0)]\n",
            "Topic 15:\n",
            "[('00', 0.0)]\n",
            "Topic 16:\n",
            "[('00', 0.0)]\n",
            "Topic 17:\n",
            "[('00', 0.0)]\n",
            "Topic 18:\n",
            "[('00', 0.0)]\n",
            "Topic 19:\n",
            "[('00', 0.0)]\n",
            "Topic 20:\n",
            "[('00', 0.0)]\n",
            "Topic 21:\n",
            "[('00', 0.0)]\n",
            "Topic 22:\n",
            "[('00', 0.0)]\n",
            "Topic 23:\n",
            "[('00', 0.0)]\n",
            "Topic 24:\n",
            "[('00', 0.0)]\n",
            "Topic 25:\n",
            "[('00', 0.0)]\n",
            "Topic 26:\n",
            "[('00', 0.0)]\n",
            "Topic 27:\n",
            "[('00', 0.0)]\n",
            "Topic 28:\n",
            "[('00', 0.0)]\n",
            "Topic 29:\n",
            "[('00', 0.0)]\n",
            "Topic 30:\n",
            "[('00', 0.0)]\n",
            "Topic 31:\n",
            "[('00', 0.0)]\n",
            "Topic 32:\n",
            "[('00', 0.0)]\n",
            "Topic 33:\n",
            "[('00', 0.0)]\n",
            "Topic 34:\n",
            "[('00', 0.0)]\n",
            "Topic 35:\n",
            "[('00', 0.0)]\n",
            "Topic 36:\n",
            "[('00', 0.0)]\n",
            "Topic 37:\n",
            "[('00', 0.0)]\n",
            "Topic 38:\n",
            "[('00', 0.0)]\n",
            "Topic 39:\n",
            "[('00', 0.0)]\n",
            "Topic 40:\n",
            "[('00', 0.0)]\n",
            "Topic 41:\n",
            "[('00', 0.0)]\n",
            "Topic 42:\n",
            "[('00', 0.0)]\n",
            "Topic 43:\n",
            "[('00', 0.0)]\n",
            "Topic 44:\n",
            "[('00', 0.0)]\n",
            "Topic 45:\n",
            "[('00', 0.0)]\n",
            "Topic 46:\n",
            "[('00', 0.0)]\n",
            "Topic 47:\n",
            "[('00', 0.0)]\n",
            "Topic 48:\n",
            "[('00', 0.0)]\n",
            "Topic 49:\n",
            "[('00', 0.0)]\n",
            "Topic 50:\n",
            "[('00', 0.0)]\n",
            "Topic 51:\n",
            "[('00', 0.0)]\n",
            "Topic 52:\n",
            "[('00', 0.0)]\n",
            "Topic 53:\n",
            "[('00', 0.0)]\n",
            "Topic 54:\n",
            "[('00', 0.0)]\n",
            "Topic 55:\n",
            "[('00', 0.0)]\n",
            "Topic 56:\n",
            "[('00', 0.0)]\n",
            "Topic 57:\n",
            "[('00', 0.0)]\n",
            "Topic 58:\n",
            "[('00', 0.0)]\n",
            "Topic 59:\n",
            "[('00', 0.0)]\n",
            "Topic 60:\n",
            "[('00', 0.0)]\n",
            "Topic 61:\n",
            "[('00', 0.0)]\n",
            "Topic 62:\n",
            "[('00', 0.0)]\n",
            "Topic 63:\n",
            "[('00', 0.0)]\n",
            "Topic 64:\n",
            "[('00', 0.0)]\n",
            "Topic 65:\n",
            "[('00', 0.0)]\n",
            "Topic 66:\n",
            "[('00', 0.0)]\n",
            "Topic 67:\n",
            "[('00', 0.0)]\n",
            "Topic 68:\n",
            "[('00', 0.0)]\n",
            "Topic 69:\n",
            "[('list', 0.0)]\n",
            "Topic 70:\n",
            "[('00', 0.0)]\n",
            "Topic 71:\n",
            "[('00', 0.0)]\n",
            "Topic 72:\n",
            "[('00', 0.0)]\n",
            "Topic 73:\n",
            "[('00', 0.0)]\n",
            "Topic 74:\n",
            "[('00', 0.0)]\n",
            "Topic 75:\n",
            "[('00', 0.0)]\n",
            "Topic 76:\n",
            "[('00', 0.0)]\n",
            "Topic 77:\n",
            "[('00', 0.0)]\n",
            "Topic 78:\n",
            "[('00', 0.0)]\n",
            "Topic 79:\n",
            "[('00', 0.0)]\n",
            "Topic 80:\n",
            "[('00', 0.0)]\n",
            "Topic 81:\n",
            "[('00', 0.0)]\n",
            "Topic 82:\n",
            "[('00', 0.0)]\n",
            "Topic 83:\n",
            "[('00', 0.0)]\n",
            "Topic 84:\n",
            "[('00', 0.0)]\n",
            "Topic 85:\n",
            "[('00', 0.0)]\n",
            "Topic 86:\n",
            "[('00', 0.0)]\n",
            "Topic 87:\n",
            "[('00', 0.0)]\n",
            "Topic 88:\n",
            "[('00', 0.0)]\n",
            "Topic 89:\n",
            "[('list', 0.0)]\n",
            "Topic 90:\n",
            "[('00', 0.0)]\n",
            "Topic 91:\n",
            "[('00', 0.0)]\n",
            "Topic 92:\n",
            "[('00', 0.0)]\n",
            "Topic 93:\n",
            "[('00', 0.0)]\n",
            "Topic 94:\n",
            "[('00', 0.0)]\n",
            "Topic 95:\n",
            "[('00', 0.0)]\n",
            "Topic 96:\n",
            "[('00', 0.0)]\n",
            "Topic 97:\n",
            "[('00', 0.0)]\n",
            "Topic 98:\n",
            "[('00', 0.0)]\n",
            "Topic 99:\n",
            "[('00', 0.0)]\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48UM3nnaKJLT",
        "colab_type": "text"
      },
      "source": [
        "* display top 1o words from each topic from LDA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-29T12:19:06.842806Z",
          "start_time": "2020-04-29T12:19:06.831721Z"
        },
        "id": "hj2hnaYWKJLU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b2e883d9-5574-4a3f-9af8-d98ffe767516"
      },
      "source": [
        "print(\"LDA Model:\")\n",
        "print_topics(lda_model, cvec,top_n=1)\n",
        "print(\"=\" * 20)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LDA Model:\n",
            "Topic 0:\n",
            "[('space', 0.010000003026986485)]\n",
            "Topic 1:\n",
            "[('drive', 0.010000003272037716)]\n",
            "Topic 2:\n",
            "[('a86', 0.010000003402702063)]\n",
            "Topic 3:\n",
            "[('file', 0.010000003405676737)]\n",
            "Topic 4:\n",
            "[('best', 0.010000003193248187)]\n",
            "Topic 5:\n",
            "[('ax', 0.010000002881344876)]\n",
            "Topic 6:\n",
            "[('com', 0.010000003349943436)]\n",
            "Topic 7:\n",
            "[('com', 0.010000003298285086)]\n",
            "Topic 8:\n",
            "[('com', 0.010000003581115299)]\n",
            "Topic 9:\n",
            "[('ax', 0.010000003353988809)]\n",
            "Topic 10:\n",
            "[('15', 0.010000003062968893)]\n",
            "Topic 11:\n",
            "[('file', 0.010000003223393124)]\n",
            "Topic 12:\n",
            "[('windows', 0.010000003391136372)]\n",
            "Topic 13:\n",
            "[('file', 0.010000003377354091)]\n",
            "Topic 14:\n",
            "[('lot', 0.010000003364437769)]\n",
            "Topic 15:\n",
            "[('software', 0.010000003334393598)]\n",
            "Topic 16:\n",
            "[('state', 0.010000003440732229)]\n",
            "Topic 17:\n",
            "[('ax', 0.010000003461838674)]\n",
            "Topic 18:\n",
            "[('key', 0.010000003165455585)]\n",
            "Topic 19:\n",
            "[('better', 0.01000000328054747)]\n",
            "Topic 20:\n",
            "[('thanks', 0.010000003169070713)]\n",
            "Topic 21:\n",
            "[('g9v', 0.01000000312018099)]\n",
            "Topic 22:\n",
            "[('a86', 0.010000002831598442)]\n",
            "Topic 23:\n",
            "[('00', 0.010000003169399115)]\n",
            "Topic 24:\n",
            "[('10', 0.010000003371415898)]\n",
            "Topic 25:\n",
            "[('time', 0.01000000314289508)]\n",
            "Topic 26:\n",
            "[('g9v', 0.01000000309537307)]\n",
            "Topic 27:\n",
            "[('ax', 0.010000003256453002)]\n",
            "Topic 28:\n",
            "[('15', 0.010000002792283072)]\n",
            "Topic 29:\n",
            "[('25', 0.010000003395058275)]\n",
            "Topic 30:\n",
            "[('file', 0.010000003229669388)]\n",
            "Topic 31:\n",
            "[('file', 0.01000000297543128)]\n",
            "Topic 32:\n",
            "[('25', 0.01000000325660433)]\n",
            "Topic 33:\n",
            "[('list', 0.010000003229392776)]\n",
            "Topic 34:\n",
            "[('lot', 0.010000003138613765)]\n",
            "Topic 35:\n",
            "[('true', 0.010000003367467727)]\n",
            "Topic 36:\n",
            "[('say', 0.0100000032097771)]\n",
            "Topic 37:\n",
            "[('make', 0.010000003124202465)]\n",
            "Topic 38:\n",
            "[('15', 0.010000003156857108)]\n",
            "Topic 39:\n",
            "[('say', 0.010000003063768987)]\n",
            "Topic 40:\n",
            "[('line', 0.010000003321958677)]\n",
            "Topic 41:\n",
            "[('25', 0.010000003112175984)]\n",
            "Topic 42:\n",
            "[('jesus', 0.010000003243866514)]\n",
            "Topic 43:\n",
            "[('ax', 0.010000003239680843)]\n",
            "Topic 44:\n",
            "[('sure', 0.010000003101202198)]\n",
            "Topic 45:\n",
            "[('12', 0.01000000309597867)]\n",
            "Topic 46:\n",
            "[('didn', 0.010000003087825761)]\n",
            "Topic 47:\n",
            "[('jesus', 0.01000000339028434)]\n",
            "Topic 48:\n",
            "[('mail', 0.01000000295155583)]\n",
            "Topic 49:\n",
            "[('ax', 0.010000002884650309)]\n",
            "Topic 50:\n",
            "[('jesus', 0.010000003339375792)]\n",
            "Topic 51:\n",
            "[('need', 0.010000002902652651)]\n",
            "Topic 52:\n",
            "[('run', 0.01000000340722107)]\n",
            "Topic 53:\n",
            "[('year', 0.01000000308139272)]\n",
            "Topic 54:\n",
            "[('need', 0.010000002870272346)]\n",
            "Topic 55:\n",
            "[('did', 0.010000003256960675)]\n",
            "Topic 56:\n",
            "[('windows', 0.010000003222371993)]\n",
            "Topic 57:\n",
            "[('key', 0.010000002870468523)]\n",
            "Topic 58:\n",
            "[('program', 0.010000003029835856)]\n",
            "Topic 59:\n",
            "[('government', 0.010000003397581747)]\n",
            "Topic 60:\n",
            "[('use', 0.010000003198714587)]\n",
            "Topic 61:\n",
            "[('software', 0.010000003118427449)]\n",
            "Topic 62:\n",
            "[('world', 0.010000002968209208)]\n",
            "Topic 63:\n",
            "[('windows', 0.010000003244590149)]\n",
            "Topic 64:\n",
            "[('don', 0.010000002884039834)]\n",
            "Topic 65:\n",
            "[('tell', 0.010000002677935603)]\n",
            "Topic 66:\n",
            "[('program', 0.010000003149904235)]\n",
            "Topic 67:\n",
            "[('course', 0.01000000314310219)]\n",
            "Topic 68:\n",
            "[('windows', 0.010000003269267376)]\n",
            "Topic 69:\n",
            "[('drive', 0.010000003200311398)]\n",
            "Topic 70:\n",
            "[('different', 0.01000000294556394)]\n",
            "Topic 71:\n",
            "[('b8f', 0.010000003348810923)]\n",
            "Topic 72:\n",
            "[('doesn', 0.010000003012718483)]\n",
            "Topic 73:\n",
            "[('20', 0.010000003372797724)]\n",
            "Topic 74:\n",
            "[('20', 0.01000000364606701)]\n",
            "Topic 75:\n",
            "[('jesus', 0.01000000338180976)]\n",
            "Topic 76:\n",
            "[('lot', 0.010000003395924432)]\n",
            "Topic 77:\n",
            "[('people', 0.01000000344207298)]\n",
            "Topic 78:\n",
            "[('going', 0.010000003328505638)]\n",
            "Topic 79:\n",
            "[('information', 0.010000003374527106)]\n",
            "Topic 80:\n",
            "[('a86', 0.010000003073823498)]\n",
            "Topic 81:\n",
            "[('like', 0.010000002938389369)]\n",
            "Topic 82:\n",
            "[('little', 0.010000003138727825)]\n",
            "Topic 83:\n",
            "[('mr', 0.01000000317195427)]\n",
            "Topic 84:\n",
            "[('right', 0.010000003397111676)]\n",
            "Topic 85:\n",
            "[('does', 0.010000003370924078)]\n",
            "Topic 86:\n",
            "[('long', 0.010000002964317143)]\n",
            "Topic 87:\n",
            "[('max', 0.010000003271480725)]\n",
            "Topic 88:\n",
            "[('used', 0.010000003329407195)]\n",
            "Topic 89:\n",
            "[('need', 0.010000003120567015)]\n",
            "Topic 90:\n",
            "[('ax', 0.010000003406053733)]\n",
            "Topic 91:\n",
            "[('help', 0.010000003002274781)]\n",
            "Topic 92:\n",
            "[('data', 0.010000003126364577)]\n",
            "Topic 93:\n",
            "[('com', 0.010000002822162164)]\n",
            "Topic 94:\n",
            "[('edu', 0.010000003483748144)]\n",
            "Topic 95:\n",
            "[('windows', 0.010000003073568676)]\n",
            "Topic 96:\n",
            "[('g9v', 0.010000003438133911)]\n",
            "Topic 97:\n",
            "[('14', 0.0100000029748679)]\n",
            "Topic 98:\n",
            "[('14', 0.010000002996526054)]\n",
            "Topic 99:\n",
            "[('government', 0.010000002945617674)]\n",
            "====================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsiLqwqiRGFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}