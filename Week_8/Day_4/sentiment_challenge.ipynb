{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600036666985",
   "display_name": "Python 3.8.5 64-bit ('boot_env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../../../large_files/output_script.txt')\n",
    "raw_smile = []\n",
    "for i in f:\n",
    "    raw_smile.append(i)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../../../large_files/output_sad.txt')\n",
    "raw_sad = []\n",
    "for i in f:\n",
    "    raw_sad.append(i)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tweets = 17000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "smile = []\n",
    "for i in raw_smile[0:n_tweets]:\n",
    "    low = i.find('\"text\": ')\n",
    "    high = i[low+9:].find('\", \"')\n",
    "    s = i[low+9:low+9+high]\n",
    "    s = re.sub(\"\\s([@][\\w_-]+)\",\"\", s)\n",
    "    s = re.sub(\"@[A-Za-z0-9_]+ \",\"\", s)\n",
    "    s = re.sub(\"RT:\",'',s)\n",
    "    pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    s = pattern.sub('', s)\n",
    "    smile.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "sad = []\n",
    "for i in raw_sad[0:n_tweets]:\n",
    "    low = i.find('\"text\": ')\n",
    "    high = i[low+9:].find('\", \"')\n",
    "    s = i[low+9:low+9+high]\n",
    "    s = re.sub(\"@[A-Za-z0-9_]+ \",\"\", s)\n",
    "    s = re.sub(\"\\s([@][\\w_-]+)\",\"\", s)\n",
    "    s = re.sub(\"RT:\",'',s)\n",
    "    pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    s = pattern.sub('', s)\n",
    "    sad.append(s)\n",
    "f = open('../../../large_files/output_sad2.txt')\n",
    "raw_sad = []\n",
    "for i in f:\n",
    "    raw_sad.append(i)\n",
    "f.close()\n",
    "remain_sad = n_tweets-len(sad)\n",
    "for i in raw_sad[0:remain_sad]:\n",
    "    low = i.find('\"text\": ')\n",
    "    high = i[low+9:].find('\", \"')\n",
    "    s = i[low+9:low+9+high]\n",
    "    s = re.sub(\"@[A-Za-z0-9_]+ \",\"\", s)\n",
    "    s = re.sub(\"\\s([@][\\w_-]+)\",\"\", s)\n",
    "    s = re.sub(\"RT:\",'',s)\n",
    "    pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    s = pattern.sub('', s)\n",
    "    sad.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for i in range(2*n_tweets):\n",
    "    if i<n_tweets:\n",
    "        y.append(1)\n",
    "    else:\n",
    "        y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = smile+sad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TDIDF vectorizer\n",
    "stopset = set(stopwords.words('english'))\n",
    "vectorizer = TfidfVectorizer(use_idf=True, lowercase=True, strip_accents='ascii', stop_words = stopset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X1,y, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "RandomForestClassifier(max_samples=0.6, n_estimators=200, n_jobs=-1)"
     },
     "metadata": {},
     "execution_count": 189
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=200,n_jobs=-1,max_samples=0.6)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9535488241989116"
     },
     "metadata": {},
     "execution_count": 190
    }
   ],
   "source": [
    "roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": ":)\n"
    }
   ],
   "source": [
    "tweet_arrays = np.array(['Best'])\n",
    "tweet_vector = vectorizer.transform(tweet_arrays)\n",
    "if clf.predict(tweet_vector):\n",
    "    print(\":)\")\n",
    "else:\n",
    "    print(\":(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}