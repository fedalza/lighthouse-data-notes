{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598886235454",
   "display_name": "Python 3.8.5 64-bit ('boot_env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['data'],data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "# Dimensionality reduction\n",
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "ridge = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Ridge()"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "ridge.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('reduce_dim', PCA()),\n",
    "    ('regressor', Ridge())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Testing score:  -7898.244407159165\n"
    }
   ],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "print('Testing score: ', pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[1.0026455 1.0026455 1.0026455 1.0026455 1.0026455 1.0026455 1.0026455\n 1.0026455 1.0026455 1.0026455 1.0026455 1.0026455 1.0026455]\n"
    }
   ],
   "source": [
    "print(pipe.steps[1][1].explained_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On every object within the pipeline the methods fit_transform are invoked during training, while transform (or predict) are called during test. So far using pipelines is just a matter of code cleaness and minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "# PCA\n",
    "n_features_to_test = np.arange(1,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization\n",
    "alpha_to_test = 2.0**np.arange(-6,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'reduce_dim__n_components': n_features_to_test,'regressor__alpha': alpha_to_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Final score is:  -6709.855911558575\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gridsearch = GridSearchCV(pipe,params, verbose=0).fit(X_train,y_train)\n",
    "print('Final score is: ', gridsearch.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'reduce_dim__n_components': 10, 'regressor__alpha': 0.015625}"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Pipeline tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers_to_test = [StandardScaler(), RobustScaler(), QuantileTransformer()]\n",
    "params = {'scaler':scalers_to_test, 'reduce_dim__n_components': n_features_to_test,'regressor__alpha': alpha_to_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, we could also apply the same approach to the dimensionality reduction step, for example to choose between PCA and SelectKBest. The only problem in this case is that PCA relies on a parameter named n_components, while SelectKBest requires to optimize a parameter named k.\n",
    "\n",
    "Luckily, GridSearchCV also allows to optimize lists of parameter dictionaries, which solves this issue as well: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "        {'scaler': scalers_to_test,\n",
    "         'reduce_dim': [PCA()],\n",
    "         'reduce_dim__n_components': n_features_to_test,\\\n",
    "         'regressor__alpha': alpha_to_test},\n",
    "\n",
    "        {'scaler': scalers_to_test,\n",
    "         'reduce_dim': [SelectKBest(f_regression)],\n",
    "         'reduce_dim__k': n_features_to_test,\\\n",
    "         'regressor__alpha': alpha_to_test}\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Fitting 5 folds for each of 720 candidates, totalling 3600 fits\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\nFinal score is : -14057.348006142514\n[Parallel(n_jobs=1)]: Done 3600 out of 3600 | elapsed:   21.4s finished\n"
    }
   ],
   "source": [
    "gridsearch = GridSearchCV(pipe,params, verbose=1).fit(X_train,y_train)\n",
    "print('Final score is :', gridsearch.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'reduce_dim': SelectKBest(k=9, score_func=<function f_regression at 0x0000013FB66B1820>),\n 'reduce_dim__k': 9,\n 'regressor__alpha': 8.0,\n 'scaler': StandardScaler()}"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}