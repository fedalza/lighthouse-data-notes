{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From `nltk` we can download translated sentences between different languages. You can see the example between **English and French** below but feel free to try different combination as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package comtrans to\n",
      "[nltk_data]     /Users/jurajkapasny/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('comtrans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AlignedSent: 'Resumption of the se...' -> 'Reprise de la sessio...'>\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import comtrans\n",
    "print(comtrans.aligned_sents('alignment-en-fr.txt')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33334"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comtrans.aligned_sents('alignment-en-fr.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginning of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename,mode='rt',encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in lines]\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Return all non-printable characters\n",
    "- Remove all punctuation characters\n",
    "- Normalize all Unicode characters to ASCII\n",
    "- Normalize the case to lowercase\n",
    "- Remove any reminaing tokens that are not alphabetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean a list of lines\n",
    "def clean_pairs(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii','ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars from each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ./english_french.pkl\n",
      "[go] => [va]\n",
      "[hi] => [salut]\n",
      "[hi] => [salut]\n",
      "[run] => [cours]\n",
      "[run] => [courez]\n",
      "[who] => [qui]\n",
      "[wow] => [ca alors]\n",
      "[fire] => [au feu]\n",
      "[help] => [a laide]\n",
      "[jump] => [saute]\n",
      "[stop] => [ca suffit]\n",
      "[stop] => [stop]\n",
      "[stop] => [arretetoi]\n",
      "[wait] => [attends]\n",
      "[wait] => [attendez]\n",
      "[go on] => [poursuis]\n",
      "[go on] => [continuez]\n",
      "[go on] => [poursuivez]\n",
      "[hello] => [bonjour]\n",
      "[hello] => [salut]\n",
      "[i see] => [je comprends]\n",
      "[i try] => [jessaye]\n",
      "[i won] => [jai gagne]\n",
      "[i won] => [je lai emporte]\n",
      "[i won] => [jai gagne]\n",
      "[oh no] => [oh non]\n",
      "[attack] => [attaque]\n",
      "[attack] => [attaquez]\n",
      "[cheers] => [sante]\n",
      "[cheers] => [a votre sante]\n",
      "[cheers] => [merci]\n",
      "[cheers] => [tchintchin]\n",
      "[get up] => [levetoi]\n",
      "[go now] => [va maintenant]\n",
      "[go now] => [allezy maintenant]\n",
      "[go now] => [vasy maintenant]\n",
      "[got it] => [jai pige]\n",
      "[got it] => [compris]\n",
      "[got it] => [pige]\n",
      "[got it] => [compris]\n",
      "[got it] => [tas capte]\n",
      "[hop in] => [monte]\n",
      "[hop in] => [montez]\n",
      "[hug me] => [serremoi dans tes bras]\n",
      "[hug me] => [serrezmoi dans vos bras]\n",
      "[i fell] => [je suis tombee]\n",
      "[i fell] => [je suis tombe]\n",
      "[i fled] => [jai fui]\n",
      "[i know] => [je sais]\n",
      "[i left] => [je suis parti]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences,filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved %s' % filename)\n",
    "\n",
    "filename = '../fra-eng/fra.txt'\n",
    "doc = load_doc(filename)\n",
    "# split into english-french pairs\n",
    "pairs = to_pairs(doc)\n",
    "# clean sentences\n",
    "clean_pairs = clean_pairs(pairs)\n",
    "# save clean pairs to file\n",
    "save_clean_data(clean_pairs, './english_french.pkl')\n",
    "# spot_check\n",
    "for i in range(50):\n",
    "    print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved english-french-both.pkl\n",
      "Saved english-french-train.pkl\n",
      "Saved english-french-test.pkl\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# load dataset\n",
    "raw_dataset = load_clean_sentences('./english_french.pkl')\n",
    "\n",
    "# reduce dataset size\n",
    "n_sentences = 10**4\n",
    "dataset = raw_dataset[:n_sentences,:]\n",
    "# random shuffle\n",
    "shuffle(dataset)\n",
    "# split into train/test\n",
    "train_len = int(0.9*len(dataset))\n",
    "train, test = dataset[:train_len], dataset[train_len:]\n",
    "# save\n",
    "save_clean_data(dataset, 'english-french-both.pkl')\n",
    "save_clean_data(train, 'english-french-train.pkl')\n",
    "save_clean_data(test, 'english-french-test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Neural Translation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages loaded\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "print('All packages loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename,'rb'))\n",
    "\n",
    "# load datasets\n",
    "dataset = load_clean_sentences('english-french-both.pkl')\n",
    "train = load_clean_sentences('english-french-train.pkl')\n",
    "test = load_clean_sentences('english-french-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary size: 2158\n",
      "English Max length: 5\n",
      "French Vocabulary size: 4388\n",
      "French Max length: 10\n"
     ]
    }
   ],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:,0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index)+1\n",
    "eng_length = max_length(dataset[:,0])\n",
    "print('English Vocabulary size: %d' % (eng_vocab_size))\n",
    "print('English Max length: %d' % (eng_length))\n",
    "# prepare french tokenizer\n",
    "fr_tokenizer = create_tokenizer(dataset[:,1])\n",
    "fr_vocab_size = len(fr_tokenizer.word_index)+1\n",
    "fr_length = max_length(dataset[:,1])\n",
    "print('French Vocabulary size: %d' % (fr_vocab_size))\n",
    "print('French Max length: %d' % (fr_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode target sequence\n",
    "def encode_ouput(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes = vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "trainX = encode_sequences(fr_tokenizer, fr_length, train[:,1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:,0])\n",
    "trainY = encode_ouput(trainY, eng_vocab_size)\n",
    "# prepare validation data\n",
    "testX = encode_sequences(fr_tokenizer,fr_length,test[:,1])\n",
    "testY = encode_sequences(eng_tokenizer,eng_length,test[:,0])\n",
    "testY = encode_ouput(testY, eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphviz\n",
      "  Downloading graphviz-0.14.1-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: graphviz\n",
      "Successfully installed graphviz-0.14.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install pydot\n",
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 10, 256)           1123328   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 5, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 5, 2158)           554606    \n",
      "=================================================================\n",
      "Total params: 2,728,558\n",
      "Trainable params: 2,728,558\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "# define NMT model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero = True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model\n",
    "\n",
    "# define model\n",
    "model = define_model(fr_vocab_size, eng_vocab_size, fr_length, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX,testY), callbacks=[checkpoint], verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
