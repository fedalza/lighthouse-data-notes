{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "mini_project_V.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz1e-j5ghc-u",
        "colab_type": "text"
      },
      "source": [
        "## Language Translator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08um_N-0h42j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f64ee287-b6fa-4a6b-881d-13bb739c0b0f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRpQCv8Vhc-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CJ4f2pRhc-_",
        "colab_type": "text"
      },
      "source": [
        "From `nltk` we can download translated sentences between different languages. You can see the example between **English and French** below but feel free to try different combination as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TDLQQQ9hc_E",
        "colab_type": "code",
        "colab": {},
        "outputId": "400c4d8a-a860-4542-c242-f9f48cb1e5e4"
      },
      "source": [
        "nltk.download('comtrans')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package comtrans to\n",
            "[nltk_data]     /Users/jurajkapasny/nltk_data...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDxanZqnhc_N",
        "colab_type": "code",
        "colab": {},
        "outputId": "a39b0fde-0116-48f8-c003-f78cd514a7fb"
      },
      "source": [
        "from nltk.corpus import comtrans\n",
        "print(comtrans.aligned_sents('alignment-en-fr.txt')[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<AlignedSent: 'Resumption of the se...' -> 'Reprise de la sessio...'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctbkqAyDhc_U",
        "colab_type": "code",
        "colab": {},
        "outputId": "552f038a-e799-45be-de93-c1410de5449c"
      },
      "source": [
        "len(comtrans.aligned_sents('alignment-en-fr.txt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rq5VBYlhc_b",
        "colab_type": "text"
      },
      "source": [
        "## Beginning of code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkMcV4eAhc_c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename,mode='rt',encoding='utf-8')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    return text"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr7NFjmchc_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "    lines = doc.strip().split('\\n')\n",
        "    pairs = [line.split('\\t') for line in lines]\n",
        "    return pairs"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7hFVxCYhc_s",
        "colab_type": "text"
      },
      "source": [
        "- Return all non-printable characters\n",
        "- Remove all punctuation characters\n",
        "- Normalize all Unicode characters to ASCII\n",
        "- Normalize the case to lowercase\n",
        "- Remove any reminaing tokens that are not alphabetic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lv4hv526hc_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "    cleaned = list()\n",
        "    # prepare regex for char filtering\n",
        "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "    # prepare translation table for removing punctuation\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    for pair in lines:\n",
        "        clean_pair = list()\n",
        "        for line in pair:\n",
        "            # normalize unicode characters\n",
        "            line = normalize('NFD', line).encode('ascii','ignore')\n",
        "            line = line.decode('UTF-8')\n",
        "            # tokenize on white space\n",
        "            line = line.split()\n",
        "            line = [word.lower() for word in line]\n",
        "            # remove punctuation from each token\n",
        "            line = [word.translate(table) for word in line]\n",
        "            # remove non-printable chars from each token\n",
        "            line = [re_print.sub('', w) for w in line]\n",
        "            # remove tokens with numbers in them\n",
        "            line = [word for word in line if word.isalpha()]\n",
        "            # store as string\n",
        "            clean_pair.append(' '.join(line))\n",
        "        cleaned.append(clean_pair)\n",
        "    return array(cleaned)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "pid7hVzFhc_1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954
        },
        "outputId": "28320e3f-bc0e-4eb1-a974-60dd06e34bd8"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences,filename):\n",
        "    dump(sentences, open(filename, 'wb'))\n",
        "    print('Saved %s' % filename)\n",
        "\n",
        "filename = './drive/My Drive/fra.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-french pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, './english_french.pkl')\n",
        "# spot_check\n",
        "for i in range(50):\n",
        "    print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved ./english_french.pkl\n",
            "[go] => [va]\n",
            "[hi] => [salut]\n",
            "[hi] => [salut]\n",
            "[run] => [cours]\n",
            "[run] => [courez]\n",
            "[who] => [qui]\n",
            "[wow] => [ca alors]\n",
            "[fire] => [au feu]\n",
            "[help] => [a laide]\n",
            "[jump] => [saute]\n",
            "[stop] => [ca suffit]\n",
            "[stop] => [stop]\n",
            "[stop] => [arretetoi]\n",
            "[wait] => [attends]\n",
            "[wait] => [attendez]\n",
            "[go on] => [poursuis]\n",
            "[go on] => [continuez]\n",
            "[go on] => [poursuivez]\n",
            "[hello] => [bonjour]\n",
            "[hello] => [salut]\n",
            "[i see] => [je comprends]\n",
            "[i try] => [jessaye]\n",
            "[i won] => [jai gagne]\n",
            "[i won] => [je lai emporte]\n",
            "[i won] => [jai gagne]\n",
            "[oh no] => [oh non]\n",
            "[attack] => [attaque]\n",
            "[attack] => [attaquez]\n",
            "[cheers] => [sante]\n",
            "[cheers] => [a votre sante]\n",
            "[cheers] => [merci]\n",
            "[cheers] => [tchintchin]\n",
            "[get up] => [levetoi]\n",
            "[go now] => [va maintenant]\n",
            "[go now] => [allezy maintenant]\n",
            "[go now] => [vasy maintenant]\n",
            "[got it] => [jai pige]\n",
            "[got it] => [compris]\n",
            "[got it] => [pige]\n",
            "[got it] => [compris]\n",
            "[got it] => [tas capte]\n",
            "[hop in] => [monte]\n",
            "[hop in] => [montez]\n",
            "[hug me] => [serremoi dans tes bras]\n",
            "[hug me] => [serrezmoi dans vos bras]\n",
            "[i fell] => [je suis tombee]\n",
            "[i fell] => [je suis tombe]\n",
            "[i fled] => [jai fui]\n",
            "[i know] => [je sais]\n",
            "[i left] => [je suis parti]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "jGP35k1ehdAA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "8a468f41-ef8a-4c26-948c-0f054f9c62cc"
      },
      "source": [
        "from pickle import load\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "    return load(open(filename, 'rb'))\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('./english_french.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = int(0.25*10**5)\n",
        "dataset = raw_dataset[:n_sentences,:]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train_len = int(0.9*len(dataset))\n",
        "train, test = dataset[:train_len], dataset[train_len:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-french-both.pkl')\n",
        "save_clean_data(train, 'english-french-train.pkl')\n",
        "save_clean_data(test, 'english-french-test.pkl')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved english-french-both.pkl\n",
            "Saved english-french-train.pkl\n",
            "Saved english-french-test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_K_zE3nhdAI",
        "colab_type": "text"
      },
      "source": [
        "## Train Neural Translation Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhvSuOddhdAI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5b3b936f-1a85-4dad-8c60-4ce495028f7c"
      },
      "source": [
        "from numpy import array\n",
        "import tensorflow as tf \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "#from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, RepeatVector, TimeDistributed\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "print('All packages loaded')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All packages loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPgjnGEQhdAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "    return load(open(filename,'rb'))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-french-both.pkl')\n",
        "train = load_clean_sentences('english-french-train.pkl')\n",
        "test = load_clean_sentences('english-french-test.pkl')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh_0gaddhdAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "    return max(len(line.split()) for line in lines)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zs53Xqs0hdAj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "9bcc1f7a-cd14-439b-f2fa-5f207381f079"
      },
      "source": [
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index)+1\n",
        "eng_length = max_length(dataset[:,0])\n",
        "print('English Vocabulary size: %d' % (eng_vocab_size))\n",
        "print('English Max length: %d' % (eng_length))\n",
        "# prepare french tokenizer\n",
        "fr_tokenizer = create_tokenizer(dataset[:,1])\n",
        "fr_vocab_size = len(fr_tokenizer.word_index)+1\n",
        "fr_length = max_length(dataset[:,1])\n",
        "print('French Vocabulary size: %d' % (fr_vocab_size))\n",
        "print('French Max length: %d' % (fr_length))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary size: 3969\n",
            "English Max length: 6\n",
            "French Vocabulary size: 7959\n",
            "French Max length: 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgR2KblBhdAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "    # integer encode sequences\n",
        "    X = tokenizer.texts_to_sequences(lines)\n",
        "    # pad sequences with 0 values\n",
        "    X = pad_sequences(X, maxlen=length, padding='post')\n",
        "    return X"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWtlMWGrhdAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one hot encode target sequence\n",
        "def encode_ouput(sequences, vocab_size):\n",
        "    ylist = list()\n",
        "    for sequence in sequences:\n",
        "        encoded = to_categorical(sequence, num_classes = vocab_size)\n",
        "        ylist.append(encoded)\n",
        "    y = array(ylist)\n",
        "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "    return y"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SICL-1PjhdA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare training data\n",
        "trainX = encode_sequences(fr_tokenizer, fr_length, train[:,1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:,0])\n",
        "trainY = encode_ouput(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(fr_tokenizer,fr_length,test[:,1])\n",
        "testY = encode_sequences(eng_tokenizer,eng_length,test[:,0])\n",
        "testY = encode_ouput(testY, eng_vocab_size)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcsSlhCyhdA5",
        "colab_type": "code",
        "colab": {},
        "outputId": "3934a4e1-037e-4072-9a97-6c7ad74389a4"
      },
      "source": [
        "#!pip install pydot\n",
        "!pip install graphviz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting graphviz\n",
            "  Downloading graphviz-0.14.1-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: graphviz\n",
            "Successfully installed graphviz-0.14.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHxbpnUAhdA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pydot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOmkXQdFhdBC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "outputId": "7b0dbec3-2514-439f-bbb0-df68696b2a85"
      },
      "source": [
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero = True))\n",
        "    model.add(LSTM(n_units))\n",
        "    model.add(RepeatVector(tar_timesteps))\n",
        "    model.add(LSTM(n_units, return_sequences=True))\n",
        "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "    return model\n",
        "\n",
        "# define model\n",
        "model = define_model(fr_vocab_size, eng_vocab_size, fr_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 12, 256)           2037504   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 6, 256)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 6, 256)            525312    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 6, 3969)           1020033   \n",
            "=================================================================\n",
            "Total params: 4,108,161\n",
            "Trainable params: 4,108,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-ce008510116f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# summarize defined model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'plot_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maNmDyHfhdBH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "02ab1629-d697-4c6f-d6d7-332709cfe8ad"
      },
      "source": [
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=50, batch_size=64, validation_data=(testX,testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.27278, saving model to model.h5\n",
            "352/352 - 12s - loss: 3.7866 - val_loss: 3.2728\n",
            "Epoch 2/50\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.27278 to 3.12541, saving model to model.h5\n",
            "352/352 - 11s - loss: 3.1510 - val_loss: 3.1254\n",
            "Epoch 3/50\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.12541 to 2.89511, saving model to model.h5\n",
            "352/352 - 11s - loss: 2.9447 - val_loss: 2.8951\n",
            "Epoch 4/50\n",
            "\n",
            "Epoch 00004: val_loss improved from 2.89511 to 2.69216, saving model to model.h5\n",
            "352/352 - 11s - loss: 2.6797 - val_loss: 2.6922\n",
            "Epoch 5/50\n",
            "\n",
            "Epoch 00005: val_loss improved from 2.69216 to 2.49845, saving model to model.h5\n",
            "352/352 - 11s - loss: 2.4299 - val_loss: 2.4984\n",
            "Epoch 6/50\n",
            "\n",
            "Epoch 00006: val_loss improved from 2.49845 to 2.33349, saving model to model.h5\n",
            "352/352 - 11s - loss: 2.1915 - val_loss: 2.3335\n",
            "Epoch 7/50\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.33349 to 2.18844, saving model to model.h5\n",
            "352/352 - 11s - loss: 1.9771 - val_loss: 2.1884\n",
            "Epoch 8/50\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.18844 to 2.08150, saving model to model.h5\n",
            "352/352 - 11s - loss: 1.7862 - val_loss: 2.0815\n",
            "Epoch 9/50\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.08150 to 1.98578, saving model to model.h5\n",
            "352/352 - 11s - loss: 1.6193 - val_loss: 1.9858\n",
            "Epoch 10/50\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.98578 to 1.91623, saving model to model.h5\n",
            "352/352 - 11s - loss: 1.4692 - val_loss: 1.9162\n",
            "Epoch 11/50\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.91623 to 1.84915, saving model to model.h5\n",
            "352/352 - 11s - loss: 1.3292 - val_loss: 1.8492\n",
            "Epoch 12/50\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.84915 to 1.78131, saving model to model.h5\n",
            "352/352 - 11s - loss: 1.2008 - val_loss: 1.7813\n",
            "Epoch 13/50\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.78131 to 1.73250, saving model to model.h5\n",
            "352/352 - 11s - loss: 1.0790 - val_loss: 1.7325\n",
            "Epoch 14/50\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.73250 to 1.69686, saving model to model.h5\n",
            "352/352 - 11s - loss: 0.9684 - val_loss: 1.6969\n",
            "Epoch 15/50\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.69686 to 1.64870, saving model to model.h5\n",
            "352/352 - 11s - loss: 0.8645 - val_loss: 1.6487\n",
            "Epoch 16/50\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.64870 to 1.62642, saving model to model.h5\n",
            "352/352 - 11s - loss: 0.7724 - val_loss: 1.6264\n",
            "Epoch 17/50\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.62642 to 1.59232, saving model to model.h5\n",
            "352/352 - 11s - loss: 0.6882 - val_loss: 1.5923\n",
            "Epoch 18/50\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.59232 to 1.57593, saving model to model.h5\n",
            "352/352 - 11s - loss: 0.6149 - val_loss: 1.5759\n",
            "Epoch 19/50\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.57593 to 1.56775, saving model to model.h5\n",
            "352/352 - 11s - loss: 0.5494 - val_loss: 1.5677\n",
            "Epoch 20/50\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.56775 to 1.55160, saving model to model.h5\n",
            "352/352 - 11s - loss: 0.4924 - val_loss: 1.5516\n",
            "Epoch 21/50\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.55160 to 1.54540, saving model to model.h5\n",
            "352/352 - 11s - loss: 0.4429 - val_loss: 1.5454\n",
            "Epoch 22/50\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.54540 to 1.53462, saving model to model.h5\n",
            "352/352 - 11s - loss: 0.3988 - val_loss: 1.5346\n",
            "Epoch 23/50\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.53462\n",
            "352/352 - 11s - loss: 0.3609 - val_loss: 1.5449\n",
            "Epoch 24/50\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.53462\n",
            "352/352 - 11s - loss: 0.3269 - val_loss: 1.5355\n",
            "Epoch 25/50\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.53462 to 1.53374, saving model to model.h5\n",
            "352/352 - 11s - loss: 0.2965 - val_loss: 1.5337\n",
            "Epoch 26/50\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.53374\n",
            "352/352 - 11s - loss: 0.2723 - val_loss: 1.5411\n",
            "Epoch 27/50\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.53374\n",
            "352/352 - 11s - loss: 0.2497 - val_loss: 1.5405\n",
            "Epoch 28/50\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.53374\n",
            "352/352 - 11s - loss: 0.2301 - val_loss: 1.5417\n",
            "Epoch 29/50\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.53374\n",
            "352/352 - 11s - loss: 0.2123 - val_loss: 1.5503\n",
            "Epoch 30/50\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.53374\n",
            "352/352 - 11s - loss: 0.1956 - val_loss: 1.5552\n",
            "Epoch 31/50\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.53374\n",
            "352/352 - 11s - loss: 0.1821 - val_loss: 1.5683\n",
            "Epoch 32/50\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.53374\n",
            "352/352 - 11s - loss: 0.1703 - val_loss: 1.5710\n",
            "Epoch 33/50\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.53374\n",
            "352/352 - 11s - loss: 0.1593 - val_loss: 1.5843\n",
            "Epoch 34/50\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.53374\n",
            "352/352 - 10s - loss: 0.1499 - val_loss: 1.5862\n",
            "Epoch 35/50\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.53374\n",
            "352/352 - 11s - loss: 0.1430 - val_loss: 1.5915\n",
            "Epoch 36/50\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.53374\n",
            "352/352 - 10s - loss: 0.1362 - val_loss: 1.5975\n",
            "Epoch 37/50\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 1.53374\n",
            "352/352 - 11s - loss: 0.1303 - val_loss: 1.6122\n",
            "Epoch 38/50\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.53374\n",
            "352/352 - 11s - loss: 0.1245 - val_loss: 1.6241\n",
            "Epoch 39/50\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.53374\n",
            "352/352 - 11s - loss: 0.1202 - val_loss: 1.6252\n",
            "Epoch 40/50\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.53374\n",
            "352/352 - 11s - loss: 0.1164 - val_loss: 1.6307\n",
            "Epoch 41/50\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.53374\n",
            "352/352 - 11s - loss: 0.1122 - val_loss: 1.6479\n",
            "Epoch 42/50\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 1.53374\n",
            "352/352 - 11s - loss: 0.1096 - val_loss: 1.6479\n",
            "Epoch 43/50\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.53374\n",
            "352/352 - 11s - loss: 0.1070 - val_loss: 1.6602\n",
            "Epoch 44/50\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.53374\n",
            "352/352 - 11s - loss: 0.1037 - val_loss: 1.6584\n",
            "Epoch 45/50\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.53374\n",
            "352/352 - 11s - loss: 0.1013 - val_loss: 1.6727\n",
            "Epoch 46/50\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.53374\n",
            "352/352 - 11s - loss: 0.0997 - val_loss: 1.6818\n",
            "Epoch 47/50\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 1.53374\n",
            "352/352 - 11s - loss: 0.0978 - val_loss: 1.6898\n",
            "Epoch 48/50\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 1.53374\n",
            "352/352 - 10s - loss: 0.0966 - val_loss: 1.7023\n",
            "Epoch 49/50\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 1.53374\n",
            "352/352 - 11s - loss: 0.0954 - val_loss: 1.7155\n",
            "Epoch 50/50\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 1.53374\n",
            "352/352 - 11s - loss: 0.0942 - val_loss: 1.7116\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc7d1fc9588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g33nknEhhdBO",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate Neural Translation model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2GJJi0lhdBO",
        "colab_type": "code",
        "colab": {},
        "outputId": "02836ac2-80d8-4be1-a399-cd88d6dc1594"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Downloading nltk-3.5.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 1.6 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk) (0.16.0)\n",
            "Collecting regex\n",
            "  Downloading regex-2020.7.14-cp38-cp38-manylinux2010_x86_64.whl (672 kB)\n",
            "\u001b[K     |████████████████████████████████| 672 kB 3.5 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from nltk) (4.48.2)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434674 sha256=74499cfb5f0215ba98189ec5a7cd920cf3acd74ad88082eadf4d2a8503e6c332\n",
            "  Stored in directory: /home/jovyan/.cache/pip/wheels/ff/d5/7b/f1fb4e1e1603b2f01c2424dd60fbcc50c12ef918bafc44b155\n",
            "Successfully built nltk\n",
            "Installing collected packages: regex, nltk\n",
            "Successfully installed nltk-3.5 regex-2020.7.14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3ylh-QJhdBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import argmax\n",
        "from tensorflow.keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-french-both.pkl')\n",
        "train = load_clean_sentences('english-french-train.pkl')\n",
        "test = load_clean_sentences('english-french-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index)+1\n",
        "eng_length = max_length(dataset[:,0])\n",
        "# prepare french tokenizer\n",
        "fr_tokenizer = create_tokenizer(dataset[:,1])\n",
        "fr_vocab_size = len(fr_tokenizer.word_index)+1\n",
        "fr_length = max_length(dataset[:,1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(fr_tokenizer, fr_length, train[:,1])\n",
        "testX = encode_sequences(fr_tokenizer, fr_length, test[:,1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsvWNrHmhdBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#translation = model.predict(source,verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ygIxt0WhdBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fiJSWIThdBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer,source): \n",
        "    prediction = model.predict(source, verbose=0)[0]\n",
        "    integers = [argmax(vector) for vector in prediction]\n",
        "    target = list()\n",
        "    for i in integers:\n",
        "        word = word_for_id(i, tokenizer)\n",
        "        if word is None:\n",
        "            break\n",
        "        target.append(word)\n",
        "    return ' '.join(target)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9e3EIg-hdBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "    actual, predicted = list(), list()\n",
        "    for i, source in enumerate(sources):\n",
        "        # translate encoded source text\n",
        "        source = source.reshape((1,source.shape[0]))\n",
        "        translation = predict_sequence(model, eng_tokenizer, source)\n",
        "        #print(raw_dataset[i])\n",
        "        raw_target, raw_src = raw_dataset[i][0], raw_dataset[i][1]\n",
        "        if i < 10:\n",
        "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "        actual.append([raw_target.split()])\n",
        "        predicted.append(translation.split())\n",
        "    # calculate Bleu score\n",
        "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0,0,0)))\n",
        "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5,0,0)))\n",
        "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3,0.3,0)))\n",
        "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25,0.25,0.25)))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei0us2bGhdBr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "outputId": "a5254421-2b92-441f-e207-52139b17118d"
      },
      "source": [
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[ne demande pas je te prie], target=[please dont ask], predicted=[please dont ask]\n",
            "src=[je vais vous attendre], target=[i can wait for you], predicted=[i will to you]\n",
            "src=[fais ce que tu veux], target=[do what you want], predicted=[do what you want]\n",
            "src=[il alla faire des emplettes], target=[he went shopping], predicted=[he went shopping]\n",
            "src=[vous en avez termine], target=[youre through], predicted=[youre through]\n",
            "src=[allumez le cierge], target=[light the candle], predicted=[light the candle]\n",
            "src=[la vie nest pas facile], target=[life aint easy], predicted=[life is easy]\n",
            "src=[cest tres collant], target=[its very sticky], predicted=[its very sticky]\n",
            "src=[je vais y aller en premier], target=[ill go first], predicted=[ill go first]\n",
            "src=[astu dormi], target=[have you slept], predicted=[have you slept]\n",
            "BLEU-1: 0.908369\n",
            "BLEU-1: 0.869415\n",
            "BLEU-1: 0.820528\n",
            "BLEU-1: 0.630555\n",
            "test\n",
            "src=[elles firent une promenade], target=[they took a walk], predicted=[they took a walk]\n",
            "src=[je commence a devenir vieux], target=[im getting old], predicted=[i grew old]\n",
            "src=[jentends le telephone], target=[i hear the phone], predicted=[i read the]\n",
            "src=[dislemoi franchement], target=[tell me frankly], predicted=[tell me frankly]\n",
            "src=[a qui fautil un verre], target=[who needs a drink], predicted=[tell wants a drink]\n",
            "src=[il me faut plus de place], target=[i need more room], predicted=[i need to]\n",
            "src=[je ne suis pas un enfant], target=[im not a child], predicted=[im not a child]\n",
            "src=[vous etes incroyable], target=[youre incredible], predicted=[youre amazing]\n",
            "src=[je me mele de ce qui ne me regarde pas], target=[im interfering], predicted=[i guess just it]\n",
            "src=[tom est saoul], target=[tom is smashed], predicted=[tom is]\n",
            "BLEU-1: 0.619460\n",
            "BLEU-1: 0.510097\n",
            "BLEU-1: 0.453851\n",
            "BLEU-1: 0.293681\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdic4xjvhdB5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "b1c9b954-24a3-47ae-a490-42b247171073"
      },
      "source": [
        "pred = encode_sequences(fr_tokenizer, fr_length, ['ca fonctionne'])\n",
        "predict_sequence(model, eng_tokenizer,pred)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'it works'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omTCpxGmhdB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}